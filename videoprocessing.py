# -*- coding: utf-8 -*-
"""VideoProcessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FYcgreC3BBebLSxT3F5OMzYQCbbMihGz
"""

!pip install opencv-python

!pip install transformers

!pip install torch

!pip install torchvision

!pip install gradio

import gradio as gr
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import cv2
import torch
import os
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

def generate_caption(frame):
    resized_frame = cv2.resize(frame, (224, 224))
    pil_image = opencv_to_pil(resized_frame)
    inputs = processor(pil_image, return_tensors="pt").to(device)
    output = model.generate(**inputs)
    caption = processor.decode(output[0], skip_special_tokens=True)
    return caption

def process_video(video_path, frame_interval=30):  # Process every nth frame
    cap = cv2.VideoCapture(video_path)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    max_seconds = 60 # Process the first 3 seconds
    frame_count = 0
    captions = []

    while cap.isOpened() and frame_count < fps * max_seconds:
        ret, frame = cap.read()
        if not ret:
            break

        # Only process every nth frame
        if frame_count % frame_interval == 0:
            caption = generate_caption(frame)
            captions.append(f"Frame {frame_count}: {caption}")

        frame_count += 1

    cap.release()
    return "\n".join(captions)  # Return captions as a string

with gr.Blocks() as interface:
    gr.Markdown("# Video Processor")
    gr.Markdown("Upload a video file to process it and generate captions for selected frames.")

    with gr.Row():
        video_input = gr.File(label="Upload a video file", file_types=["video"], type="filepath")
        output_text = gr.Textbox(label="Output", lines=10)

    # Create button to trigger processing
    process_button = gr.Button("Process Video")
    process_button.click(fn=process_video, inputs=video_input, outputs=output_text)

interface.launch()